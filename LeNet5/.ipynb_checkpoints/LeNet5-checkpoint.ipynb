{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c9c0568-b768-426c-8238-dc4dd7522196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/118], Loss: 0.8445\n",
      "Validation Loss: 0.8071, Accuracy: 76.11%\n",
      "\n",
      "Epoch [2/10], Step [82/118], Loss: 0.5903\n",
      "Validation Loss: 0.5821, Accuracy: 82.70%\n",
      "\n",
      "Epoch [3/10], Step [64/118], Loss: 0.4949\n",
      "Validation Loss: 0.4904, Accuracy: 85.54%\n",
      "\n",
      "Epoch [4/10], Step [46/118], Loss: 0.4655\n",
      "Validation Loss: 0.4340, Accuracy: 87.40%\n",
      "\n",
      "Epoch [5/10], Step [28/118], Loss: 0.3631\n",
      "Validation Loss: 0.3954, Accuracy: 88.64%\n",
      "\n",
      "Epoch [6/10], Step [10/118], Loss: 0.2520\n",
      "Validation Loss: 0.3680, Accuracy: 89.17%\n",
      "\n",
      "Epoch [6/10], Step [110/118], Loss: 0.4242\n",
      "Validation Loss: 0.3434, Accuracy: 90.00%\n",
      "\n",
      "Epoch [7/10], Step [92/118], Loss: 0.2592\n",
      "Validation Loss: 0.3251, Accuracy: 90.78%\n",
      "\n",
      "Epoch [8/10], Step [74/118], Loss: 0.2891\n",
      "Validation Loss: 0.3088, Accuracy: 91.25%\n",
      "\n",
      "Epoch [9/10], Step [56/118], Loss: 0.3050\n",
      "Validation Loss: 0.2956, Accuracy: 91.51%\n",
      "\n",
      "Epoch [10/10], Step [38/118], Loss: 0.3220\n",
      "Validation Loss: 0.2835, Accuracy: 91.89%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        #C1 (convolutional)= feature map 6, check 5x5, input image 32x32, 156 trainable parameter, 122,304 connection\n",
    "        #nn.Conv2d(batch_size, channels(==색상채널), height, width(==이미지 크기))\n",
    "        self.c1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5, 5), padding='same')  # 수정: in_channels와 out_channels 설정\n",
    "        self.relu1 = nn.ReLU()\n",
    "        #S2 (subsampling)= feature map 6, feature map size 14x14, unit receptive field는 C1 feature map에서 2x2, 12 trainable parameter, 5,880 connection\n",
    "        #nn.MaxPool2d(kernel_size= ,stride=)\n",
    "        #nn.AvgPool2d(kernel_size=, stride=)\n",
    "        self.s2 = nn.MaxPool2d(kernel_size=2, stride=2)#AvgPool을 권장하지만 이게 더 잘나옴\n",
    "        \n",
    "        #C3 (convolutional)=\n",
    "        self.c3 = nn.Conv2d(in_channels=32, out_channels=48, kernel_size=(5, 5), padding='valid')  # 수정: in_channels 설정\n",
    "        self.relu2 = nn.ReLU()\n",
    "        #S4 (subsampling)=\n",
    "        self.s4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        #C5 (convolutional)=\n",
    "        self.c5 = nn.Conv2d(in_channels=48, out_channels=120, kernel_size=5)\n",
    "        \n",
    "        #F6 (fully ..)=84 units, C5 fully connected, 10,164 trainable parameter\n",
    "        self.f6 = nn.Linear(120, 84)  # 수정: 입력 크기 변경\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.f7 = nn.Linear(84, 10)  # 최종 출력 클래스 수 (10개)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.c1(x)\n",
    "        x=self.s2(x)\n",
    "        x=self.c3(x)\n",
    "        x=self.s4(x)\n",
    "        x=self.c5(x).view(-1,120)\n",
    "        x=self.f6(x)\n",
    "        x=self.f7(x)\n",
    "        return x\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        dtable = scipy.io.loadmat('MNIST.mat')\n",
    "        self.data = np.transpose(dtable['data']).astype(np.float32)\n",
    "        self.label = np.transpose(dtable['label']).squeeze()  # 1D로 변환\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx].reshape(1, 28, 28)\n",
    "        label = torch.tensor(self.label[idx].astype(np.int64), dtype=torch.long)\n",
    "        return torch.tensor(data), label\n",
    "\n",
    "# Define your model\n",
    "model = MyModel()\n",
    "train_dataset, valid_dataset = random_split(MyCustomDataset(), [60000, 10000])\n",
    "train_loader, valid_loader = DataLoader(train_dataset, 512, shuffle=True), DataLoader(valid_dataset, 512, shuffle=False)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "running_loss = 0.0\n",
    "step = 0\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        model.train()  # Set model to training mode\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        # Log training status\n",
    "        if step % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch_idx + 1}/{len(train_loader)}], '\n",
    "                  f'Loss: {loss.item():.4f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_loss = 0.0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for inputs, targets in valid_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, targets)\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "                    # For classification tasks, calculate accuracy\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += targets.size(0)\n",
    "                    correct += (predicted == targets).sum().item()\n",
    "\n",
    "                avg_valid_loss = valid_loss / len(valid_loader)\n",
    "                accuracy = 100 * correct / total\n",
    "                print(f'Validation Loss: {avg_valid_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b6716-7af0-443f-af57-0ce0aa7ef8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "        #nn.Linear(input 차원, output vector차원)\n",
    "        #nn.Conv1d(batch_size, sequence_length, channels(==input의 feautre num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ed713a1-7309-4bbb-99b3-75bf3909665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/118], Loss: 1.7872\n",
      "Validation Loss: 1.7489, Accuracy: 54.69%\n",
      "\n",
      "Epoch [2/10], Step [82/118], Loss: 1.0305\n",
      "Validation Loss: 1.2097, Accuracy: 67.07%\n",
      "\n",
      "Epoch [3/10], Step [64/118], Loss: 0.8337\n",
      "Validation Loss: 0.9885, Accuracy: 72.38%\n",
      "\n",
      "Epoch [4/10], Step [46/118], Loss: 0.7705\n",
      "Validation Loss: 0.8635, Accuracy: 75.81%\n",
      "\n",
      "Epoch [5/10], Step [28/118], Loss: 0.7035\n",
      "Validation Loss: 0.7810, Accuracy: 77.93%\n",
      "\n",
      "Epoch [6/10], Step [10/118], Loss: 0.7356\n",
      "Validation Loss: 0.7192, Accuracy: 79.39%\n",
      "\n",
      "Epoch [6/10], Step [110/118], Loss: 0.6283\n",
      "Validation Loss: 0.6744, Accuracy: 80.68%\n",
      "\n",
      "Epoch [7/10], Step [92/118], Loss: 0.6159\n",
      "Validation Loss: 0.6379, Accuracy: 81.78%\n",
      "\n",
      "Epoch [8/10], Step [74/118], Loss: 0.6101\n",
      "Validation Loss: 0.6072, Accuracy: 82.70%\n",
      "\n",
      "Epoch [9/10], Step [56/118], Loss: 0.5477\n",
      "Validation Loss: 0.5829, Accuracy: 83.48%\n",
      "\n",
      "Epoch [10/10], Step [38/118], Loss: 0.4957\n",
      "Validation Loss: 0.5601, Accuracy: 83.97%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # 첫 번째 완전 연결 층\n",
    "        self.fc2 = nn.Linear(128, 64)        # 두 번째 완전 연결 층\n",
    "        self.fc3 = nn.Linear(64, 10)         # 출력 층 (10 클래스)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # 입력을 1D 벡터로 변환\n",
    "        x = torch.relu(self.fc1(x))  # 첫 번째 층에 ReLU 적용\n",
    "        x = torch.relu(self.fc2(x))  # 두 번째 층에 ReLU 적용\n",
    "        x = self.fc3(x)              # 출력 층\n",
    "        return x\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        dtable = scipy.io.loadmat('MNIST.mat')\n",
    "        self.data = np.transpose(dtable['data']).astype(np.float32)\n",
    "        self.label = np.transpose(dtable['label']).squeeze()  # 1D로 변환\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx].reshape(1, 28, 28)\n",
    "        label = torch.tensor(self.label[idx].astype(np.int64), dtype=torch.long)\n",
    "        return torch.tensor(data), label\n",
    "\n",
    "# Define your model\n",
    "model = MyModel()\n",
    "train_dataset, valid_dataset = random_split(MyCustomDataset(), [60000, 10000])\n",
    "train_loader, valid_loader = DataLoader(train_dataset, 512, shuffle=True), DataLoader(valid_dataset, 512, shuffle=False)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "running_loss = 0.0\n",
    "step = 0\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        model.train()  # Set model to training mode\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        # Log training status\n",
    "        if step % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch_idx + 1}/{len(train_loader)}], '\n",
    "                  f'Loss: {loss.item():.4f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_loss = 0.0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for inputs, targets in valid_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, targets)\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "                    # For classification tasks, calculate accuracy\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += targets.size(0)\n",
    "                    correct += (predicted == targets).sum().item()\n",
    "\n",
    "                avg_valid_loss = valid_loss / len(valid_loader)\n",
    "                accuracy = 100 * correct / total\n",
    "                print(f'Validation Loss: {avg_valid_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b4ef7-6251-4ec7-9dd2-c4cda78fe230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

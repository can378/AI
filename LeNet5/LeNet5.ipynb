{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9c0568-b768-426c-8238-dc4dd7522196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/118], Loss: 0.9506\n",
      "Validation Loss: 0.8107, Accuracy: 77.11%\n",
      "\n",
      "Epoch [2/10], Step [82/118], Loss: 0.6521\n",
      "Validation Loss: 0.5913, Accuracy: 83.19%\n",
      "\n",
      "Epoch [3/10], Step [64/118], Loss: 0.4913\n",
      "Validation Loss: 0.4861, Accuracy: 86.00%\n",
      "\n",
      "Epoch [4/10], Step [46/118], Loss: 0.3244\n",
      "Validation Loss: 0.4400, Accuracy: 87.45%\n",
      "\n",
      "Epoch [5/10], Step [28/118], Loss: 0.4709\n",
      "Validation Loss: 0.3950, Accuracy: 88.68%\n",
      "\n",
      "Epoch [6/10], Step [10/118], Loss: 0.3236\n",
      "Validation Loss: 0.3677, Accuracy: 89.50%\n",
      "\n",
      "Epoch [6/10], Step [110/118], Loss: 0.3954\n",
      "Validation Loss: 0.3460, Accuracy: 90.18%\n",
      "\n",
      "Epoch [7/10], Step [92/118], Loss: 0.2027\n",
      "Validation Loss: 0.3311, Accuracy: 90.36%\n",
      "\n",
      "Epoch [8/10], Step [74/118], Loss: 0.3064\n",
      "Validation Loss: 0.3123, Accuracy: 91.14%\n",
      "\n",
      "Epoch [9/10], Step [56/118], Loss: 0.3266\n",
      "Validation Loss: 0.2981, Accuracy: 91.51%\n",
      "\n",
      "Epoch [10/10], Step [38/118], Loss: 0.4094\n",
      "Validation Loss: 0.2875, Accuracy: 91.64%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import sys\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        #kaggle 참조후 변환\n",
    "        #C1 (convolutional)= 6 feature map(28x28), check 5x5, input image channel =1-grayscal images, 156 trainable parameter, 122,304 connection\n",
    "        #nn.Conv2d(batch_size, channels(==색상채널), height, width(==이미지 크기), kernel_size=, padding=)\n",
    "        #nn.Conv2d(in_channels=input channel num, out_channels=out channel num, kernel_size=check neighbors, \n",
    "        self.c1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5, 5), padding='same')\n",
    "        self.relu1 = nn.ReLU()#activation\n",
    "        \n",
    "        #S2 (subsampling)= 6 feature map (14x14), kernel size 2x2, stride=2 default, 12 trainable parameter, 5,880 connection\n",
    "        #nn.MaxPool2d(kernel_size= ,stride=)    nn.AvgPool2d(kernel_size=, stride=)\n",
    "        self.s2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        #C3 (convolutional)= 16 feature maps, kernel size(5x5), \n",
    "        self.c3 = nn.Conv2d(in_channels=32, out_channels=48, kernel_size=(5, 5), padding='valid')\n",
    "        self.relu2 = nn.ReLU()#activation\n",
    "\n",
    "        #S4 (subsampling)= 16 feature maps(5x5), kernel size(2x2) #stride=2 default\n",
    "        self.s4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        #C5 (convolutional)= 120 feature maps, kernel size(5x5), 45 120 trainable connections\n",
    "        self.c5 = nn.Conv2d(in_channels=48, out_channels=120, kernel_size=(5,5))\n",
    "        self.relu3 = nn.ReLU()\n",
    "       \n",
    "        #F6 (fully ..)=84 units, (fully connected to C5), 10,164 trainable parameter\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        #F7\n",
    "        self.f7 = nn.Linear(84, 10)  # 클래스 수 10개\n",
    "        self.softmax1 = nn.Softmax(dim=1)\n",
    "\n",
    "        '''\n",
    "        #kernel(5) filter가 feature extract. 4x4작아짐\n",
    "        self.c1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.s2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.c3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.s4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.c5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5)\n",
    "        self.f6 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.f7 = nn.Linear(in_features=84, out_features=10)\n",
    "        '''\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.c1(x)\n",
    "        x=self.s2(x)\n",
    "        x=self.c3(x)\n",
    "        x=self.s4(x)\n",
    "        x=self.c5(x).view(-1,120)#flatten\n",
    "        x=self.f6(x)\n",
    "        x=self.f7(x)\n",
    "        return x\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        dtable = scipy.io.loadmat('MNIST.mat')\n",
    "        self.data = np.transpose(dtable['data']).astype(np.float32)\n",
    "        self.label = np.transpose(dtable['label']).squeeze()  # 1D로 변경\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx].reshape(1, 28, 28)\n",
    "        label = torch.tensor(self.label[idx].astype(np.int64), dtype=torch.long)\n",
    "        return torch.tensor(data), label\n",
    "\n",
    "# Define your model\n",
    "model = MyModel()\n",
    "train_dataset, valid_dataset = random_split(MyCustomDataset(), [60000, 10000])\n",
    "train_loader, valid_loader = DataLoader(train_dataset, 512, shuffle=True), DataLoader(valid_dataset, 512, shuffle=False)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "running_loss = 0.0\n",
    "step = 0\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        model.train()  # Set model to training mode\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        # Log training status\n",
    "        if step % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch_idx + 1}/{len(train_loader)}], '\n",
    "                  f'Loss: {loss.item():.4f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_loss = 0.0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for inputs, targets in valid_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, targets)\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "                    # For classification tasks, calculate accuracy\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += targets.size(0)\n",
    "                    correct += (predicted == targets).sum().item()\n",
    "\n",
    "                avg_valid_loss = valid_loss / len(valid_loader)\n",
    "                accuracy = 100 * correct / total\n",
    "                print(f'Validation Loss: {avg_valid_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b6716-7af0-443f-af57-0ce0aa7ef8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "        #nn.Linear(input 차원, output vector차원)\n",
    "        #nn.Conv1d(batch_size, sequence_length, channels(==input의 feautre num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ed713a1-7309-4bbb-99b3-75bf3909665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/118], Loss: 0.8891\n",
      "Validation Loss: 0.8603, Accuracy: 75.36%\n",
      "\n",
      "Epoch [2/10], Step [82/118], Loss: 0.5239\n",
      "Validation Loss: 0.6255, Accuracy: 82.25%\n",
      "\n",
      "Epoch [3/10], Step [64/118], Loss: 0.5534\n",
      "Validation Loss: 0.5272, Accuracy: 84.98%\n",
      "\n",
      "Epoch [4/10], Step [46/118], Loss: 0.3427\n",
      "Validation Loss: 0.4707, Accuracy: 86.62%\n",
      "\n",
      "Epoch [5/10], Step [28/118], Loss: 0.4264\n",
      "Validation Loss: 0.4292, Accuracy: 87.88%\n",
      "\n",
      "Epoch [6/10], Step [10/118], Loss: 0.3974\n",
      "Validation Loss: 0.3975, Accuracy: 88.69%\n",
      "\n",
      "Epoch [6/10], Step [110/118], Loss: 0.3432\n",
      "Validation Loss: 0.3722, Accuracy: 89.37%\n",
      "\n",
      "Epoch [7/10], Step [92/118], Loss: 0.4481\n",
      "Validation Loss: 0.3511, Accuracy: 90.02%\n",
      "\n",
      "Epoch [8/10], Step [74/118], Loss: 0.3108\n",
      "Validation Loss: 0.3332, Accuracy: 90.50%\n",
      "\n",
      "Epoch [9/10], Step [56/118], Loss: 0.3172\n",
      "Validation Loss: 0.3170, Accuracy: 90.92%\n",
      "\n",
      "Epoch [10/10], Step [38/118], Loss: 0.2169\n",
      "Validation Loss: 0.3045, Accuracy: 91.32%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "import torch\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import sys\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        #kaggle 참조후 변환\n",
    "        #C1 (convolutional)= 6 feature map(28x28), check 5x5, input image channel =1-grayscal images, 156 trainable parameter, 122,304 connection\n",
    "        #nn.Conv2d(batch_size, channels(==색상채널), height, width(==이미지 크기), kernel_size=, padding=)\n",
    "        #nn.Conv2d(in_channels=input channel num, out_channels=out channel num, kernel_size=check neighbors, \n",
    "        self.c1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5, 5), padding='same')\n",
    "        self.relu1 = nn.ReLU()#activation\n",
    "        \n",
    "        #S2 (subsampling)= 6 feature map (14x14), kernel size 2x2, stride=2 default, 12 trainable parameter, 5,880 connection\n",
    "        #nn.MaxPool2d(kernel_size= ,stride=)    nn.AvgPool2d(kernel_size=, stride=)\n",
    "        self.s2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        #C3 (convolutional)= 16 feature maps, kernel size(5x5), \n",
    "        self.c3 = nn.Conv2d(in_channels=32, out_channels=48, kernel_size=(5, 5), padding='valid')\n",
    "        self.relu2 = nn.ReLU()#activation\n",
    "\n",
    "        #S4 (subsampling)= 16 feature maps(5x5), kernel size(2x2) #stride=2 default\n",
    "        self.s4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        #C5 (convolutional)= 120 feature maps, kernel size(5x5), 45 120 trainable connections\n",
    "        self.c5 = nn.Conv2d(in_channels=48, out_channels=120, kernel_size=(5,5))\n",
    "        self.relu3 = nn.ReLU()\n",
    "       \n",
    "        #F6 (fully ..)=84 units, (fully connected to C5), 10,164 trainable parameter\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        #F7\n",
    "        self.f7 = nn.Linear(84, 10)  # 클래스 수 10개\n",
    "        self.softmax1 = nn.Softmax(dim=1)\n",
    "\n",
    "        '''\n",
    "        #kernel(5) filter가 feature extract. 4x4작아짐\n",
    "        self.c1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.s2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.c3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.s4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.c5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5)\n",
    "        self.f6 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.f7 = nn.Linear(in_features=84, out_features=10)\n",
    "        '''\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.c1(x)\n",
    "        x=self.s2(x)\n",
    "        x=self.c3(x)\n",
    "        x=self.s4(x)\n",
    "        x=self.c5(x).view(-1,120)#flatten\n",
    "        x=self.f6(x)\n",
    "        x=self.f7(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_list (list): List of data samples (e.g., file paths or data arrays).\n",
    "            targets (list or array, optional): Corresponding labels or targets for supervised learning.\n",
    "            transform (callable, optional): A function/transform to apply to each sample.\n",
    "        \"\"\"\n",
    "        dtable = scipy.io.loadmat('MNIST.mat')\n",
    "        self.data = np.transpose(dtable['data']).astype(np.float32)\n",
    "        self.label = np.transpose(dtable['label'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx].reshape(1,28,28)\n",
    "        label = torch.tensor(self.label[idx], dtype=torch.long).squeeze()\n",
    "        return data, label\n",
    "\n",
    "# Define your model (replace 'MyModel' with your actual model class)\n",
    "model = MyModel()\n",
    "train_dataset, valid_dataset = random_split(MyCustomDataset(), [60000, 10000])\n",
    "train_loader, valid_loader = DataLoader(train_dataset, 512, shuffle=True), DataLoader(train_dataset, 512, shuffle=False)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()  # Replace with your chosen loss function\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)  # Replace with your optimizer and hyperparameters\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "running_loss = 0.0\n",
    "step = 0\n",
    "num_epochs = 10\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        model.train()  # Set model to training mode\n",
    "        # Move data to the appropriate device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        # Log training status\n",
    "        if step%100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch_idx + 1}/{len(train_loader)}], '\n",
    "                  f'Loss: {loss.item():.4f}')\n",
    "\n",
    "            with torch.no_grad():  # Disable gradient computation\n",
    "                valid_loss = 0.0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for inputs, targets in valid_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, targets)\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "                    # For classification tasks, calculate accuracy\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += targets.size(0)\n",
    "                    correct += (predicted == targets).sum().item()\n",
    "\n",
    "            avg_valid_loss = valid_loss / len(valid_loader)\n",
    "            accuracy = 100 * correct / total\n",
    "            print(f'Validation Loss: {avg_valid_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c7b4ef7-6251-4ec7-9dd2-c4cda78fe230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/118], Loss: 1.4594\n",
      "Validation Loss: 1.6158, Accuracy: 58.92%\n",
      "\n",
      "Epoch [2/10], Step [82/118], Loss: 1.1554\n",
      "Validation Loss: 1.0690, Accuracy: 70.20%\n",
      "\n",
      "Epoch [3/10], Step [64/118], Loss: 0.8509\n",
      "Validation Loss: 0.8609, Accuracy: 75.65%\n",
      "\n",
      "Epoch [4/10], Step [46/118], Loss: 0.6452\n",
      "Validation Loss: 0.7479, Accuracy: 78.69%\n",
      "\n",
      "Epoch [5/10], Step [28/118], Loss: 0.5645\n",
      "Validation Loss: 0.6718, Accuracy: 80.83%\n",
      "\n",
      "Epoch [6/10], Step [10/118], Loss: 0.5740\n",
      "Validation Loss: 0.6215, Accuracy: 82.18%\n",
      "\n",
      "Epoch [6/10], Step [110/118], Loss: 0.6133\n",
      "Validation Loss: 0.5777, Accuracy: 83.38%\n",
      "\n",
      "Epoch [7/10], Step [92/118], Loss: 0.5791\n",
      "Validation Loss: 0.5432, Accuracy: 84.31%\n",
      "\n",
      "Epoch [8/10], Step [74/118], Loss: 0.5097\n",
      "Validation Loss: 0.5160, Accuracy: 85.03%\n",
      "\n",
      "Epoch [9/10], Step [56/118], Loss: 0.4300\n",
      "Validation Loss: 0.4933, Accuracy: 85.67%\n",
      "\n",
      "Epoch [10/10], Step [38/118], Loss: 0.4502\n",
      "Validation Loss: 0.4725, Accuracy: 86.16%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#original paper\n",
    "import torch\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import sys\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.c1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.s2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.c3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.s4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.c5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=3)\n",
    "        self.f6 = nn.Linear(480, 84)\n",
    "        self.f7 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.s2(x)\n",
    "        x = self.c3(x)\n",
    "        x = self.s4(x)\n",
    "        x = self.c5(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.f6(x)\n",
    "        x = self.f7(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_list (list): List of data samples (e.g., file paths or data arrays).\n",
    "            targets (list or array, optional): Corresponding labels or targets for supervised learning.\n",
    "            transform (callable, optional): A function/transform to apply to each sample.\n",
    "        \"\"\"\n",
    "        dtable = scipy.io.loadmat('MNIST.mat')\n",
    "        self.data = np.transpose(dtable['data']).astype(np.float32)\n",
    "        self.label = np.transpose(dtable['label'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx].reshape(1,28,28)\n",
    "        label = torch.tensor(self.label[idx], dtype=torch.long).squeeze()\n",
    "        return data, label\n",
    "\n",
    "# Define your model (replace 'MyModel' with your actual model class)\n",
    "model = MyModel()\n",
    "train_dataset, valid_dataset = random_split(MyCustomDataset(), [60000, 10000])\n",
    "train_loader, valid_loader = DataLoader(train_dataset, 512, shuffle=True), DataLoader(train_dataset, 512, shuffle=False)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()  # Replace with your chosen loss function\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)  # Replace with your optimizer and hyperparameters\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "running_loss = 0.0\n",
    "step = 0\n",
    "num_epochs = 10\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        model.train()  # Set model to training mode\n",
    "        # Move data to the appropriate device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        # Log training status\n",
    "        if step%100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch_idx + 1}/{len(train_loader)}], '\n",
    "                  f'Loss: {loss.item():.4f}')\n",
    "\n",
    "            with torch.no_grad():  # Disable gradient computation\n",
    "                valid_loss = 0.0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for inputs, targets in valid_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, targets)\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "                    # For classification tasks, calculate accuracy\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += targets.size(0)\n",
    "                    correct += (predicted == targets).sum().item()\n",
    "\n",
    "            avg_valid_loss = valid_loss / len(valid_loader)\n",
    "            accuracy = 100 * correct / total\n",
    "            print(f'Validation Loss: {avg_valid_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c89a8ca-eaf2-484a-93dd-e76df2f7510a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
